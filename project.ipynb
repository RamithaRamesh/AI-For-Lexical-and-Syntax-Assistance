{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79d4a21a",
      "metadata": {
        "id": "79d4a21a"
      },
      "source": [
        "# Question 1: AI for Lexical and Syntax Assistance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d584c689",
      "metadata": {
        "id": "d584c689"
      },
      "source": [
        "Creating typo-correction pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15f48ef",
      "metadata": {
        "id": "c15f48ef",
        "outputId": "ac7e5656-b9b9-42bf-a0a7-7ad594ae147d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import string\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "keyboard_neighbors = {\n",
        "    'a': 'qwsz', 'b': 'vghn', 'c': 'xdfv', 'd': 'serfcx',\n",
        "    'e': 'wsdr', 'f': 'rtgvcd', 'g': 'tyhbvf', 'h': 'yujnbg',\n",
        "    'i': 'ujko', 'j': 'uikmnh', 'k': 'iolmj', 'l': 'kop',\n",
        "    'm': 'njk', 'n': 'bhjm', 'o': 'iklp', 'p': 'ol', 'q': 'wa',\n",
        "    'r': 'edft', 's': 'awedxz', 't': 'rfgy', 'u': 'yhji',\n",
        "    'v': 'cfgb', 'w': 'qesa', 'x': 'zsdc', 'y': 'tghu', 'z': 'asx'\n",
        "}\n",
        "\n",
        "ALPHABET = string.ascii_lowercase\n",
        "\n",
        "\n",
        "def adaptive_typo_count(word, min_per_word=30, max_per_word=500):\n",
        "    \"\"\"\n",
        "    Determine how many unique typos to attempt for `word`.\n",
        "    Short words get fewer, long identifiers get many.\n",
        "    Tunable via min_per_word / max_per_word.\n",
        "    \"\"\"\n",
        "    length = len(word)\n",
        "    if length <= 2:\n",
        "        return min_per_word // 4\n",
        "    elif length <= 4:\n",
        "        return min_per_word\n",
        "    elif length <= 8:\n",
        "        return min(min_per_word * 4, max_per_word // 4)\n",
        "    elif length <= 15:\n",
        "        return min(min_per_word * 10, max_per_word // 2)\n",
        "    else:\n",
        "        return max_per_word\n",
        "\n",
        "def replace_with_neighbor(ch):\n",
        "    \"\"\"Replace character with one of its keyboard neighbors (if present).\"\"\"\n",
        "    lower = ch.lower()\n",
        "    if lower in keyboard_neighbors and keyboard_neighbors[lower]:\n",
        "        choice = random.choice(keyboard_neighbors[lower])\n",
        "        # preserve case\n",
        "        return choice.upper() if ch.isupper() else choice\n",
        "    else:\n",
        "        return random.choice(ALPHABET)\n",
        "\n",
        "def random_case_transform(s):\n",
        "    \"\"\"Randomly toggle case of one or more characters (useful for identifiers).\"\"\"\n",
        "    if len(s) == 0:\n",
        "        return s\n",
        "    s = list(s)\n",
        "    i = random.randrange(len(s))\n",
        "    s[i] = s[i].upper() if s[i].islower() else s[i].lower()\n",
        "    return \"\".join(s)\n",
        "\n",
        "def generate_typo(word):\n",
        "    \"\"\"\n",
        "    Create a single realistic typo variant of `word`.\n",
        "    Uses multiple strategies; returns a string (possibly equal to input rarely).\n",
        "    \"\"\"\n",
        "    if not word:\n",
        "        return word\n",
        "\n",
        "    ops = [\n",
        "        \"swap_adjacent\",\n",
        "        \"delete\", \n",
        "        \"insert_neighbor\", \n",
        "        \"replace_neighbor\",\n",
        "        \"replace_random\", \n",
        "        \"double_char\", \n",
        "        \"transpose\", \n",
        "        \"case_change\"      \n",
        "    ]\n",
        "    op = random.choice(ops)\n",
        "    s = list(word)\n",
        "\n",
        "    try:\n",
        "        if op == \"swap_adjacent\" and len(s) > 1:\n",
        "            i = random.randint(0, len(s) - 2)\n",
        "            s[i], s[i+1] = s[i+1], s[i]\n",
        "\n",
        "        elif op == \"delete\" and len(s) > 0:\n",
        "            i = random.randint(0, len(s) - 1)\n",
        "            del s[i]\n",
        "\n",
        "        elif op == \"insert_neighbor\":\n",
        "            i = random.randint(0, len(s))\n",
        "\n",
        "            if len(word) > 0:\n",
        "                source_index = max(0, min(len(s)-1, i-1))\n",
        "                ch = s[source_index]\n",
        "                ins = replace_with_neighbor(ch)\n",
        "            else:\n",
        "                ins = random.choice(ALPHABET)\n",
        "            s.insert(i, ins)\n",
        "\n",
        "        elif op == \"replace_neighbor\" and len(s) > 0:\n",
        "            i = random.randint(0, len(s) - 1)\n",
        "            s[i] = replace_with_neighbor(s[i])\n",
        "\n",
        "        elif op == \"replace_random\" and len(s) > 0:\n",
        "            i = random.randint(0, len(s) - 1)\n",
        "            s[i] = random.choice(ALPHABET.upper() if s[i].isupper() else ALPHABET)\n",
        "\n",
        "        elif op == \"double_char\" and len(s) > 0:\n",
        "            i = random.randint(0, len(s) - 1)\n",
        "            s.insert(i, s[i])\n",
        "\n",
        "        elif op == \"transpose\" and len(s) > 1:\n",
        "            i = random.randint(0, len(s) - 1)\n",
        "            j = random.randint(0, len(s) - 1)\n",
        "            if i != j:\n",
        "                s[i], s[j] = s[j], s[i]\n",
        "\n",
        "        elif op == \"case_change\":\n",
        "            return random_case_transform(word)\n",
        "\n",
        "        if random.random() < 0.12 and len(s) > 0:\n",
        "            j = random.choice([\"delete\", \"double_char\", \"replace_neighbor\", \"insert_neighbor\"])\n",
        "            if j == \"delete\" and len(s) > 0:\n",
        "                idx = random.randint(0, len(s)-1)\n",
        "                del s[idx]\n",
        "            elif j == \"double_char\" and len(s) > 0:\n",
        "                idx = random.randint(0, len(s)-1)\n",
        "                s.insert(idx, s[idx])\n",
        "            elif j == \"replace_neighbor\" and len(s) > 0:\n",
        "                idx = random.randint(0, len(s)-1)\n",
        "                s[idx] = replace_with_neighbor(s[idx])\n",
        "            elif j == \"insert_neighbor\":\n",
        "                idx = random.randint(0, len(s))\n",
        "                src = s[max(0, min(len(s)-1, idx-1))] if s else random.choice(ALPHABET)\n",
        "                s.insert(idx, replace_with_neighbor(src))\n",
        "\n",
        "    except Exception:\n",
        "        if len(word) > 0:\n",
        "            i = random.randint(0, len(word)-1)\n",
        "            s = list(word)\n",
        "            s[i] = random.choice(ALPHABET)\n",
        "\n",
        "    typo = \"\".join(s)\n",
        "\n",
        "    if typo == word:\n",
        "        if len(word) > 0:\n",
        "            i = random.randint(0, len(word)-1)\n",
        "            s = list(word)\n",
        "            s[i] = random.choice(ALPHABET)\n",
        "            typo = \"\".join(s)\n",
        "\n",
        "    return typo\n",
        "\n",
        "\n",
        "def make_typos_for_word(word, target_count, safety_cap=5000):\n",
        "    \"\"\"\n",
        "    Generate up to target_count unique typos for a single word.\n",
        "    Uses a safety_cap to stop infinite loops.\n",
        "    \"\"\"\n",
        "    typos = set()\n",
        "    attempts = 0\n",
        "    while len(typos) < target_count and attempts < safety_cap:\n",
        "        t = generate_typo(word)\n",
        "        if t and t != word:\n",
        "            typos.add(t)\n",
        "        attempts += 1\n",
        "    return list(typos)\n",
        "\n",
        "def generate_typos_for_language(lang, keywords, min_per_word=30, max_per_word=500):\n",
        "    \"\"\"\n",
        "    For each keyword, generate an adaptive # of typos and return list of (typo, correct).\n",
        "    \"\"\"\n",
        "    all_pairs = []\n",
        "    for word in keywords:\n",
        "        target = adaptive_typo_count(word, min_per_word, max_per_word)\n",
        "        typos = make_typos_for_word(word, target, safety_cap=target * 20)\n",
        "        for t in typos:\n",
        "            all_pairs.append((t, word))\n",
        "    return all_pairs\n",
        "\n",
        "\n",
        "LANG_KEYWORDS = {\n",
        "    \"python\": [\n",
        "        \"def\", \"return\", \"if\", \"else\", \"elif\", \"for\", \"while\", \"break\", \"continue\",\n",
        "        \"import\", \"from\", \"as\", \"class\", \"try\", \"except\", \"lambda\", \"global\", \"with\",\n",
        "        \"print\", \"input\", \"open\", \"read\", \"write\", \"strip\", \"split\", \"join\", \"format\",\n",
        "        \"self\", \"__init__\", \"__str__\", \"len\", \"range\", \"enumerate\", \"map\", \"filter\"\n",
        "    ],\n",
        "    \"java\": [\n",
        "        \"public\", \"class\", \"static\", \"void\", \"main\", \"String\", \"if\", \"else\", \"for\",\n",
        "        \"while\", \"return\", \"int\", \"float\", \"boolean\", \"try\", \"catch\", \"import\", \"package\",\n",
        "        \"new\", \"this\", \"extends\", \"implements\", \"throws\", \"interface\", \"System.out.println\"\n",
        "    ],\n",
        "    \"c\": [\n",
        "        \"int\", \"float\", \"if\", \"else\", \"for\", \"while\", \"return\", \"printf\", \"scanf\",\n",
        "        \"include\", \"define\", \"char\", \"void\", \"main\", \"switch\", \"case\", \"break\",\n",
        "        \"#include\", \"malloc\", \"free\", \"sizeof\", \"struct\", \"typedef\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "OUTPUT_DIR = \"typo_datasets\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "MIN_PER_WORD = 30 \n",
        "MAX_PER_WORD = 600 \n",
        "\n",
        "for lang, keywords in LANG_KEYWORDS.items():\n",
        "    print(f\"\\nGenerating typos for {lang} ...\")\n",
        "    pairs = generate_typos_for_language(lang, keywords, min_per_word=MIN_PER_WORD, max_per_word=MAX_PER_WORD)\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    csv_path = os.path.join(OUTPUT_DIR, f\"typo_data_{lang}.csv\")\n",
        "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"typo\", \"correct\"])\n",
        "        writer.writerows(pairs)\n",
        "\n",
        "    print(f\"âœ… Saved {len(pairs)} pairs to {csv_path}\")\n",
        "\n",
        "print(\"\\nAll CSV files created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef368982",
      "metadata": {
        "id": "ef368982"
      },
      "source": [
        "Encoding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88dd2e16",
      "metadata": {
        "id": "88dd2e16"
      },
      "outputs": [],
      "source": [
        "LOWER_CASE_LETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "UPPER_CASE_LETTERS = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
        "DIGITS = list(\"0123456789\")\n",
        "SPECIAL_CHARS = ['_', '-', '.','@','|','(',')','~','!','#','$','%','*','-','+','{','}','[',']','\\\\n',':',';','\"','\\'','<','>',',','.','=','`']\n",
        "\n",
        "SPECIAL_TOKENS = ['<pad>', '<sos>', '<eos>']\n",
        "\n",
        "VOCAB = SPECIAL_TOKENS + LOWER_CASE_LETTERS + DIGITS + SPECIAL_CHARS\n",
        "\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(VOCAB)}\n",
        "itos = {i: ch for i, ch in enumerate(VOCAB)}\n",
        "\n",
        "def encode_string(word):\n",
        "    \"\"\"\n",
        "    Converts a word (string) into a list of integers.\n",
        "    Adds <sos> at the start and <eos> at the end.\n",
        "    Unknown characters are converted to <pad>.\n",
        "    \"\"\"\n",
        "    encoded = [stoi['<sos>']]\n",
        "    for ch in word:\n",
        "        encoded.append(stoi.get(ch, stoi['<pad>'])) \n",
        "    encoded.append(stoi['<eos>'])\n",
        "    return encoded\n",
        "\n",
        "def decode_string(indices):\n",
        "    \"\"\"\n",
        "    Converts a list of numeric indices back into a readable word.\n",
        "    Ignores special tokens (<sos>, <eos>, <pad>).\n",
        "    \"\"\"\n",
        "    chars = [itos[i] for i in indices if itos[i] not in ['<sos>', '<eos>', '<pad>']]\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae789c3d",
      "metadata": {
        "id": "ae789c3d"
      },
      "source": [
        "Integrating both the above cells to prepare the data for the Nueral Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc501197",
      "metadata": {
        "id": "cc501197",
        "outputId": "360ecaf7-a4a6-4930-abfc-0610d2f89491"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "DATA_DIR = \"typo_datasets\"\n",
        "OUTPUT_DIR = \"processed\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "LANGS = [\"python\", \"java\", \"c\"]\n",
        "MAX_LEN = 32 \n",
        "\n",
        "def build_vocab_from_csv(csv_path):\n",
        "    chars = set()\n",
        "\n",
        "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)\n",
        "        for typo, correct in reader:\n",
        "            for ch in typo.strip():\n",
        "                chars.add(ch)\n",
        "            for ch in correct.strip():\n",
        "                chars.add(ch)\n",
        "\n",
        "    chars = sorted(list(chars))\n",
        "\n",
        "    vocab = [\"<pad>\", \"<sos>\", \"<eos>\"] + chars\n",
        "\n",
        "    stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "    itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "    return stoi, itos\n",
        "\n",
        "def encode_string(text, stoi):\n",
        "    \"\"\"Convert string into [<sos>, chars..., <eos>]\"\"\"\n",
        "    ids = [stoi[\"<sos>\"]]\n",
        "    for ch in text:\n",
        "        ids.append(stoi.get(ch, stoi[\"<pad>\"]))\n",
        "    ids.append(stoi[\"<eos>\"])\n",
        "    return ids\n",
        "\n",
        "def pad_sequence(seq, max_len, pad_idx):\n",
        "    if len(seq) < max_len:\n",
        "        return seq + [pad_idx] * (max_len - len(seq))\n",
        "    else:\n",
        "        return seq[:max_len]\n",
        "\n",
        "for lang in LANGS:\n",
        "    print(f\"\\nProcessing {lang.upper()} dataset...\")\n",
        "\n",
        "    csv_path = os.path.join(DATA_DIR, f\"typo_data_{lang}.csv\")\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"File not found: {csv_path}\")\n",
        "        continue\n",
        "\n",
        "    stoi, itos = build_vocab_from_csv(csv_path)\n",
        "    pad_idx = stoi[\"<pad>\"]\n",
        "\n",
        "    print(f\"Vocabulary size ({lang}):\", len(stoi))\n",
        "\n",
        "    np.save(os.path.join(OUTPUT_DIR, f\"{lang}_stoi.npy\"), stoi)\n",
        "    np.save(os.path.join(OUTPUT_DIR, f\"{lang}_itos.npy\"), itos)\n",
        "\n",
        "    typo_pairs = []\n",
        "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)\n",
        "        typo_pairs = [(t.strip(), c.strip()) for t, c in reader]\n",
        "\n",
        "\n",
        "    X_list = []\n",
        "    Y_in_list = []\n",
        "    Y_out_list = []\n",
        "\n",
        "    for typo, correct in typo_pairs:\n",
        "        enc_typo = encode_string(typo, stoi)\n",
        "        enc_correct = encode_string(correct, stoi)\n",
        "\n",
        "        dec_in = enc_correct[:-1]\n",
        "        dec_out = enc_correct[1:]\n",
        "\n",
        "        X_list.append(pad_sequence(enc_typo, MAX_LEN, pad_idx))\n",
        "        Y_in_list.append(pad_sequence(dec_in, MAX_LEN, pad_idx))\n",
        "        Y_out_list.append(pad_sequence(dec_out, MAX_LEN, pad_idx))\n",
        "\n",
        "    X = np.array(X_list, dtype=np.int32)\n",
        "    Y_in = np.array(Y_in_list, dtype=np.int32)\n",
        "    Y_out = np.array(Y_out_list, dtype=np.int32)\n",
        "\n",
        "    np.save(os.path.join(OUTPUT_DIR, f\"{lang}_X.npy\"), X)\n",
        "    np.save(os.path.join(OUTPUT_DIR, f\"{lang}_Yin.npy\"), Y_in)\n",
        "    np.save(os.path.join(OUTPUT_DIR, f\"{lang}_Yout.npy\"), Y_out)\n",
        "\n",
        "    print(f\"âœ” Saved {lang} arrays:\")\n",
        "    print(\"  X    :\", X.shape)\n",
        "    print(\"  Y_in :\", Y_in.shape)\n",
        "    print(\"  Y_out:\", Y_out.shape)\n",
        "\n",
        "print(\"\\nAll languages processed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec78ecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04cd07c",
      "metadata": {
        "id": "a04cd07c"
      },
      "source": [
        "LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4c3144",
      "metadata": {
        "id": "3d4c3144",
        "outputId": "da925cb2-d68c-4797-8e29-7cd1fe99c090"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, LSTM, Embedding, Dense, Dropout,\n",
        "    TimeDistributed, Attention, Concatenate\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "\n",
        "\n",
        "@register_keras_serializable()\n",
        "class CastToFloat32(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.cast(inputs, tf.float32)\n",
        "\n",
        "\n",
        "def pad_sequence(seq, max_len, pad_idx):\n",
        "    return seq + [pad_idx] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "\n",
        "\n",
        "def decode_sequence(id_list, itos):\n",
        "    chars = []\n",
        "    for token_id in id_list:\n",
        "        ch = itos[token_id]\n",
        "        if ch in [\"<pad>\", \"<sos>\", \"<eos>\"]:\n",
        "            continue\n",
        "        chars.append(ch)\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def encode_string(text, stoi):\n",
        "    ids = [stoi[\"<sos>\"]]\n",
        "    for ch in text:\n",
        "        ids.append(stoi.get(ch, stoi[\"<pad>\"]))\n",
        "    ids.append(stoi[\"<eos>\"])\n",
        "    return ids\n",
        "\n",
        "def edit_distance(a, b):\n",
        "    la, lb = len(a), len(b)\n",
        "    dp = [[0]*(lb+1) for _ in range(la+1)]\n",
        "    for i in range(la+1): dp[i][0] = i\n",
        "    for j in range(lb+1): dp[0][j] = j\n",
        "\n",
        "    for i in range(1, la+1):\n",
        "        for j in range(1, lb+1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j]+1,\n",
        "                dp[i][j-1]+1,\n",
        "                dp[i-1][j-1]+cost\n",
        "            )\n",
        "    return dp[la][lb]\n",
        "\n",
        "\n",
        "LANG_KEYWORDS = {\n",
        "    \"python\": [\n",
        "        \"def\",\"return\",\"if\",\"else\",\"elif\",\"for\",\"while\",\"break\",\"continue\",\n",
        "        \"import\",\"from\",\"as\",\"class\",\"try\",\"except\",\"lambda\",\"global\",\"with\",\n",
        "        \"print\",\"input\",\"len\",\"range\",\"int\",\"float\",\"str\",\"True\",\"False\",\"None\"\n",
        "    ],\n",
        "    \"java\": [\n",
        "        \"public\",\"class\",\"static\",\"void\",\"main\",\"String\",\"if\",\"else\",\"for\",\n",
        "        \"while\",\"return\",\"int\",\"float\",\"boolean\",\"try\",\"catch\",\"import\",\"package\"\n",
        "    ],\n",
        "    \"c\": [\n",
        "        \"int\",\"float\",\"if\",\"else\",\"for\",\"while\",\"return\",\"printf\",\"scanf\",\n",
        "        \"define\",\"char\",\"void\",\"main\",\"switch\",\"case\",\"break\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def nearest_keyword(word, lang_keywords, max_dist=2):\n",
        "    wl = word.lower()\n",
        "    best = word\n",
        "    best_d = 999\n",
        "    for kw in lang_keywords:\n",
        "        d = edit_distance(wl, kw.lower())\n",
        "        if d < best_d:\n",
        "            best_d = d\n",
        "            best = kw\n",
        "    return best if best_d <= max_dist else word\n",
        "\n",
        "\n",
        "def build_model(vocab_size, max_len, embedding_dim=256, lstm_units=384):\n",
        "    encoder_input = Input(shape=(max_len,), name=\"encoder_input\")\n",
        "    encoder_emb = Embedding(vocab_size, embedding_dim)(encoder_input)\n",
        "    encoder_emb = Dropout(0.3)(encoder_emb)\n",
        "\n",
        "    encoder_outputs, state_h, state_c = LSTM(\n",
        "        lstm_units, return_sequences=True, return_state=True\n",
        "    )(encoder_emb)\n",
        "\n",
        "    encoder_outputs = CastToFloat32()(encoder_outputs)\n",
        "\n",
        "    decoder_input = Input(shape=(max_len,), name=\"decoder_input\")\n",
        "    decoder_emb = Embedding(vocab_size, embedding_dim)(decoder_input)\n",
        "\n",
        "    decoder_outputs, _, _ = LSTM(\n",
        "        lstm_units, return_sequences=True, return_state=True\n",
        "    )(decoder_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    decoder_outputs = CastToFloat32()(decoder_outputs)\n",
        "\n",
        "    attention = Attention()([decoder_outputs, encoder_outputs])\n",
        "    context = Concatenate()([decoder_outputs, attention])\n",
        "\n",
        "    output = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(context)\n",
        "\n",
        "    model = Model([encoder_input, decoder_input], output)\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_typo(model, word, stoi, itos, max_len, lang_keywords):\n",
        "    encoded = encode_string(word, stoi)\n",
        "    enc_padded = np.array([pad_sequence(encoded, max_len, stoi[\"<pad>\"])])\n",
        "\n",
        "    target_seq = np.array([[stoi[\"<sos>\"]]])\n",
        "    decoded_tokens = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        dec_padded = np.array([pad_sequence(target_seq[0].tolist(), max_len, stoi[\"<pad>\"])])\n",
        "        preds = model.predict([enc_padded, dec_padded], verbose=0)\n",
        "\n",
        "        token_id = np.argmax(preds[0, len(target_seq[0]) - 1])\n",
        "        token = itos[token_id]\n",
        "\n",
        "        if token == \"<eos>\":\n",
        "            break\n",
        "        if token != \"<pad>\":\n",
        "            decoded_tokens.append(token)\n",
        "\n",
        "        target_seq = np.append(target_seq, [[token_id]], axis=1)\n",
        "\n",
        "    pred = ''.join(decoded_tokens)\n",
        "    return nearest_keyword(pred, lang_keywords)\n",
        "\n",
        "\n",
        "LANGS = [\"python\", \"java\", \"c\"]\n",
        "\n",
        "for lang in LANGS:\n",
        "    print(f\"TRAINING LSTM MODEL FOR: {lang.upper()}\")\n",
        "\n",
        "    X = np.load(f\"processed/{lang}_X.npy\")\n",
        "    Y_in = np.load(f\"processed/{lang}_Yin.npy\")\n",
        "    Y_out = np.load(f\"processed/{lang}_Yout.npy\")\n",
        "\n",
        "    stoi = np.load(f\"processed/{lang}_stoi.npy\", allow_pickle=True).item()\n",
        "    itos = np.load(f\"processed/{lang}_itos.npy\", allow_pickle=True).item()\n",
        "\n",
        "    vocab_size = len(stoi)\n",
        "    max_len = X.shape[1]\n",
        "\n",
        "    words_raw = []\n",
        "    for seq in Y_out:\n",
        "        words_raw.append(decode_sequence(seq, itos))\n",
        "\n",
        "\n",
        "    X_train, X_test, Y_in_train, Y_in_test, Y_out_train, Y_out_test, words_raw_train, words_raw_test = train_test_split(\n",
        "        X, Y_in, Y_out, words_raw, test_size=0.15, random_state=42\n",
        "    )\n",
        "\n",
        "    Y_out_train_exp = np.expand_dims(Y_out_train, -1)\n",
        "    Y_out_test_exp = np.expand_dims(Y_out_test, -1)\n",
        "\n",
        "    model = build_model(vocab_size, max_len)\n",
        "\n",
        "    early = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "    lr_reduce = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "\n",
        "    model.fit(\n",
        "        [X_train, Y_in_train],\n",
        "        Y_out_train_exp,\n",
        "        validation_split=0.2,\n",
        "        batch_size=32,\n",
        "        epochs=30,\n",
        "        callbacks=[early, lr_reduce],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    save_path = f\"models/{lang}_lstm_typo_corrector.keras\"\n",
        "    model.save(save_path)\n",
        "    print(f\"Saved model: {save_path}\")\n",
        "\n",
        "    loss, acc = model.evaluate([X_test, Y_in_test], Y_out_test_exp, verbose=0)\n",
        "    print(f\"Token Accuracy ({lang}): {acc:.4f}\")\n",
        "\n",
        "\n",
        "    correct = 0\n",
        "    for w in words_raw_test:\n",
        "        if predict_typo(model, w, stoi, itos, max_len, LANG_KEYWORDS[lang]) == w:\n",
        "            correct += 1\n",
        "\n",
        "    print(f\"Word Accuracy ({lang}): {correct/len(words_raw_test):.4f}\")\n",
        "\n",
        "\n",
        "    print(\"\\nSAMPLE PREDICTIONS:\")\n",
        "    test_words = [\"pritn\", \"whiel\", \"retunr\", \"flase\", \"ture\", \"inptu\"]\n",
        "    for w in test_words:\n",
        "        try:\n",
        "            print(f\"{w:12s} â†’ {predict_typo(model, w, stoi, itos, max_len, LANG_KEYWORDS[lang])}\")\n",
        "        except:\n",
        "            print(f\"{w:12s} â†’ ERROR\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc22adf",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pandas\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefae644",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "DATASET_DIR = \"typo_datasets\"\n",
        "LANGUAGES = [\"python\", \"java\", \"c\"]\n",
        "MODEL_DIR = \"keras_models\"\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e660a497",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "DATASET_DIR = \"./typo_datasets\"\n",
        "MODEL_DIR = \"./models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "LANG_FILES = {\n",
        "    \"c\": \"typo_data_c.csv\",\n",
        "    \"java\": \"typo_data_java.csv\",\n",
        "    \"python\": \"typo_data_python.csv\",\n",
        "}\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = np.zeros((position, d_model), dtype=np.float32)\n",
        "    positions = np.arange(position)[:, np.newaxis]\n",
        "    dims = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / d_model)\n",
        "    angle_rads = positions * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], tf.float32)\n",
        "\n",
        "\n",
        "def build_vocab(df):\n",
        "    chars = sorted(set(\"\".join(df['typo']) + \"\".join(df['correct'])))\n",
        "    chars = [\"<pad>\", \"<start>\", \"<end>\"] + chars\n",
        "    char2idx = {c: i for i, c in enumerate(chars)}\n",
        "    idx2char = {i: c for i, c in enumerate(chars)}\n",
        "    return char2idx, idx2char\n",
        "\n",
        "\n",
        "def encode_texts(texts, char2idx, max_len):\n",
        "    pad = char2idx[\"<pad>\"]\n",
        "    start = char2idx[\"<start>\"]\n",
        "    end = char2idx[\"<end>\"]\n",
        "\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        seq = [start] + [char2idx.get(c, pad) for c in str(t)] + [end]\n",
        "        seq = seq[:max_len] + [pad] * max(0, max_len - len(seq))\n",
        "        encoded.append(seq)\n",
        "    return np.array(encoded)\n",
        "\n",
        "\n",
        "def decode(ids, idx2char):\n",
        "    return \"\".join([idx2char[i] for i in ids if idx2char[i] not in (\"<pad>\", \"<start>\", \"<end>\")])\n",
        "\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.l1 = LayerNormalization()\n",
        "        self.l2 = LayerNormalization()\n",
        "        self.d1 = Dropout(0.1)\n",
        "        self.d2 = Dropout(0.1)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        att_out = self.att(x, x)\n",
        "        att_out = self.d1(att_out, training=training)\n",
        "        out1 = self.l1(x + att_out)\n",
        "        ffn_out = self.ffn(out1)\n",
        "        ffn_out = self.d2(ffn_out, training=training)\n",
        "        return self.l2(out1 + ffn_out)\n",
        "\n",
        "\n",
        "class CharacterTransformer(Model):\n",
        "    def __init__(self, vocab_size, embed_dim=128, heads=4, ff_dim=256, layers=2, max_pos=100):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.heads = heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_layers = layers\n",
        "        self.max_pos = max_pos\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.pos = positional_encoding(max_pos, embed_dim)\n",
        "\n",
        "        self.blocks = [TransformerBlock(embed_dim, heads, ff_dim) for _ in range(layers)]\n",
        "        self.final = Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x) + self.pos[:, :length, :]\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, training=training)\n",
        "        return self.final(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"heads\": self.heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"layers\": self.num_layers,\n",
        "            \"max_pos\": self.max_pos,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg):\n",
        "        return cls(**cfg)\n",
        "\n",
        "def train_language(lang, file_name):\n",
        "    print(f\"\\nTraining {lang}...\")\n",
        "\n",
        "    df = pd.read_csv(os.path.join(DATASET_DIR, file_name))\n",
        "    df['typo'] = df['typo'].astype(str)\n",
        "    df['correct'] = df['correct'].astype(str)\n",
        "\n",
        "    char2idx, idx2char = build_vocab(df)\n",
        "    vocab = len(char2idx)\n",
        "\n",
        "    max_len = max(df['typo'].str.len().max(), df['correct'].str.len().max()) + 2\n",
        "\n",
        "    X = encode_texts(df[\"typo\"], char2idx, max_len)\n",
        "    Y = encode_texts(df[\"correct\"], char2idx, max_len)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(32)\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "\n",
        "    model = CharacterTransformer(vocab, max_pos=max_len)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optim = tf.keras.optimizers.Adam()\n",
        "\n",
        "    train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for epoch in range(1, 31):\n",
        "        train_acc.reset_state()\n",
        "        test_acc.reset_state()\n",
        "        total_loss = 0\n",
        "        batches = 0\n",
        "\n",
        "        for inp, tar in train_ds:\n",
        "            with tf.GradientTape() as tape:\n",
        "                pred = model(inp, training=True)\n",
        "                L = loss(tar, pred)\n",
        "\n",
        "            grads = tape.gradient(L, model.trainable_variables)\n",
        "            optim.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            train_acc.update_state(tar, pred)\n",
        "            total_loss += float(L)\n",
        "            batches += 1\n",
        "\n",
        "        for inp, tar in test_ds:\n",
        "            pred = model(inp, training=False)\n",
        "            test_acc.update_state(tar, pred)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Loss:{total_loss/batches:.4f} | Train:{train_acc.result()*100:.2f}% | Test:{test_acc.result()*100:.2f}%\")\n",
        "\n",
        "\n",
        "    model_path = os.path.join(MODEL_DIR, f\"{lang}_typer.keras\")\n",
        "    model.save(model_path)\n",
        "    print(f\"Saved model â†’ {model_path}\")\n",
        "\n",
        "\n",
        "    meta_path = os.path.join(MODEL_DIR, f\"{lang}_typer.metadata.npz\")\n",
        "    np.savez(meta_path, vocab=np.array(list(char2idx.keys())), max_len=max_len)\n",
        "    print(f\"Saved metadata â†’ {meta_path}\")\n",
        "\n",
        "\n",
        "    settings_path = os.path.join(MODEL_DIR, f\"{lang}_model_settings.json\")\n",
        "    with open(settings_path, \"w\") as f:\n",
        "        json.dump({\"char2idx\": char2idx, \"max_len\": int(max_len)}, f, indent=4)\n",
        "\n",
        "    print(f\"Saved settings â†’ {settings_path}\")\n",
        "\n",
        "\n",
        "for lang, file_name in LANG_FILES.items():\n",
        "    train_language(lang, file_name)\n",
        "\n",
        "\n",
        "def predict_language_word(lang, word):\n",
        "    model_path = f\"{MODEL_DIR}/{lang}_typer.keras\"\n",
        "    meta_path = f\"{MODEL_DIR}/{lang}_typer.metadata.npz\"\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Model not found.\")\n",
        "        return\n",
        "\n",
        "    model = tf.keras.models.load_model(\n",
        "        model_path,\n",
        "        custom_objects={\"CharacterTransformer\": CharacterTransformer}\n",
        "    )\n",
        "\n",
        "    meta = np.load(meta_path, allow_pickle=True)\n",
        "    chars = list(meta[\"vocab\"])\n",
        "    max_len = int(meta[\"max_len\"])\n",
        "\n",
        "    char2idx = {c: i for i, c in enumerate(chars)}\n",
        "    idx2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "    inp = encode_texts([word], char2idx, max_len)\n",
        "    logits = model(inp, training=False)\n",
        "\n",
        "    pred = tf.argmax(logits[0], axis=-1).numpy()\n",
        "    result = decode(pred, idx2char)\n",
        "\n",
        "    print(f\"{word} â†’ {result}\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3476a333",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe81b425",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, SimpleRNN, Dense, Dropout,\n",
        "    Bidirectional, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "LANGS = [\"python\", \"java\", \"c\"]\n",
        "def pad_sequence(seq, max_len, pad_idx):\n",
        "    return seq + [pad_idx] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "\n",
        "\n",
        "def encode_string(text, stoi):\n",
        "    ids = [stoi[\"<sos>\"]]\n",
        "    for ch in text:\n",
        "        ids.append(stoi.get(ch, stoi[\"<pad>\"]))\n",
        "    ids.append(stoi[\"<eos>\"])\n",
        "    return ids\n",
        "\n",
        "\n",
        "def decode_sequence(id_list, itos):\n",
        "    chars = []\n",
        "    for token_id in id_list:\n",
        "        ch = itos[token_id]\n",
        "        if ch not in [\"<pad>\", \"<sos>\", \"<eos>\"]:\n",
        "            chars.append(ch)\n",
        "    return ''.join(chars)\n",
        "\n",
        "for lang in LANGS:\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"TRAINING RNN MODEL FOR LANGUAGE: {lang.upper()}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X = np.load(f\"processed/{lang}_X.npy\")\n",
        "    Y_out = np.load(f\"processed/{lang}_Yout.npy\")\n",
        "    stoi = np.load(f\"processed/{lang}_stoi.npy\", allow_pickle=True).item()\n",
        "    itos = np.load(f\"processed/{lang}_itos.npy\", allow_pickle=True).item()\n",
        "\n",
        "    vocab_size = len(stoi)\n",
        "    max_seq_len = X.shape[1]\n",
        "\n",
        "    Y_expanded = np.expand_dims(Y_out, -1)\n",
        "\n",
        "    print(f\"Loaded shapes â†’ X: {X.shape}, Y: {Y_expanded.shape}\")\n",
        "\n",
        "    embedding_dim = 128\n",
        "    hidden_units = 256\n",
        "    dropout_rate = 0.25\n",
        "    learning_rate = 0.0005\n",
        "\n",
        "    inputs = Input(shape=(max_seq_len,), name=\"input_layer\")\n",
        "\n",
        "    x = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        mask_zero=True,\n",
        "        name=\"embedding_layer\"\n",
        "    )(inputs)\n",
        "\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = Bidirectional(SimpleRNN(\n",
        "        hidden_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_dropout=0.2,\n",
        "    ), name=\"bidir_rnn_1\")(x)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = SimpleRNN(\n",
        "        hidden_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_dropout=0.2,\n",
        "        name=\"rnn_layer_2\"\n",
        "    )(x)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "    rnn_model = Model(inputs, outputs, name=f\"RNN_{lang}\")\n",
        "\n",
        "    rnn_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    print(rnn_model.summary())\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            patience=8,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\",\n",
        "            factor=0.5,\n",
        "            patience=4,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = rnn_model.fit(\n",
        "        X, Y_expanded,\n",
        "        batch_size=64,\n",
        "        epochs=30,\n",
        "        validation_split=0.15,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    save_path = f\"models/{lang}_rnn_typo_corrector.keras\"\n",
        "    rnn_model.save(save_path)\n",
        "\n",
        "    print(f\"\\nSaved model to {save_path}\")\n",
        "\n",
        "    def predict_correction(typo):\n",
        "        encoded = pad_sequence(encode_string(typo, stoi), max_seq_len, stoi[\"<pad>\"])\n",
        "        X_input = np.array([encoded])\n",
        "\n",
        "        preds = rnn_model.predict(X_input, verbose=0)\n",
        "        pred_ids = np.argmax(preds[0], axis=-1)\n",
        "\n",
        "        out_chars = [\n",
        "            itos[i] for i in pred_ids\n",
        "            if itos[i] not in [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "        ]\n",
        "        return \"\".join(out_chars)\n",
        "\n",
        "    print(\"\\nðŸ”Ž SAMPLE TESTS\")\n",
        "    test_typos = [\"pritn\", \"whiel\", \"reutrn\", \"fo\", \"esle\", \"inpt\", \"rnage\"]\n",
        "    for typo in test_typos:\n",
        "        print(f\"{typo:<12} â†’ {predict_correction(typo)}\")\n",
        "\n",
        "    val_split = int(len(X) * 0.85)\n",
        "    X_val = X[val_split:]\n",
        "    Y_val = Y_expanded[val_split:]\n",
        "\n",
        "    val_loss, val_acc = rnn_model.evaluate(X_val, Y_val, verbose=0)\n",
        "\n",
        "    print(\"\\nVALIDATION RESULTS\")\n",
        "    print(f\"Loss:      {val_loss:.4f}\")\n",
        "    print(f\"Accuracy:  {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL LANGUAGES TRAINED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
